{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59f5bafa",
   "metadata": {},
   "source": [
    "# <span style=\"color:purple; font-weight:bold\"> Principal Component Anaylisis           \n",
    "\n",
    "- **PCA is a dimensionality reduction technique.**  \n",
    "                  \n",
    "- **In real life the datasets used in models have large number of features (dimensions) all of which contribute to predicting target variable.**   \n",
    "\n",
    "- **PCA reduces the number of features by finding new axes (called principal components) that capture the maximum variance in the data.**        \n",
    "\n",
    "- **Think of it as to “Rotate the dataset onto directions where the data varies the most, then keep only the strongest directions.”**             \n",
    "\n",
    "\n",
    "### <span style=\"color:purple; font-weight:bold\">Advantages ?</span>        \n",
    "\n",
    "* Speeds up ML models\n",
    "\n",
    "* Removes noise\n",
    "\n",
    "* Avoids multicollinearity\n",
    "\n",
    "* Helps visualize high-dimensional data\n",
    "\n",
    "* Makes patterns easier to see              \n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:purple; font-weight:bold\">Maths</span>     \n",
    "\n",
    "Below are the mathemetical topics you must be familiar with to understand PCA.          \n",
    "\n",
    "- <e style=\"color:purple; font-weight:bold\">Statistic</e>\n",
    "   * <e style=\"color:blue; font-weight:bold\">Mean/Average:</e> Calculating the mean of each feature is the first step in centering the data, a critical preprocessing step for PCA.      \n",
    "\n",
    "   * <e style=\"color:blue; font-weight:bold\">Variance:</e> This measures the spread of data points along a single dimension. PCA's primary goal is to find new axes (principal components) that maximize this variance.      \n",
    "\n",
    "   * <e style=\"color:blue; font-weight:bold\">Covariance:</e> This measures the degree to which two variables change together. PCA aims to transform correlated features into new, uncorrelated components (zero covariance).    \n",
    "\n",
    "   * <e style=\"color:blue; font-weight:bold\">Covariance Matrix:</e> This symmetric matrix summarizes all the pairwise covariances and individual variances of your dataset's features. It is the central piece of the PCA calculation.      \n",
    "\n",
    "   \n",
    "- <e style=\"color:purple; font-weight:bold\">Linear Alegebra</e>\n",
    "   * <e style=\"color:blue; font-weight:bold\">Scalars, Vectors, and Matrices:</e> Basic familiarity with these data structures and their operations (addition, multiplication, transpose) is essential for handling data manipulation in the algorithm.             \n",
    "\n",
    "   * <e style=\"color:blue; font-weight:bold\">Matrix Operations:</e> Understanding matrix multiplication and transposition is necessary for working with the data matrix and the covariance matrix.   \n",
    "\n",
    "   * <e style=\"color:blue; font-weight:bold\">Eigenvectors:</e> represent the directions of the new axes (principal components) where the data has the most variance.      \n",
    "\n",
    "   * <e style=\"color:blue; font-weight:bold\">Eigenvalues:</e> indicate the magnitude of the variance along each eigenvector's direction. Larger eigenvalues mean more information is captured by that component.            \n",
    "\n",
    "   * <e style=\"color:blue; font-weight:bold\">Eigendecomposition:</e> This is the process of breaking down the covariance matrix into its eigenvectors and eigenvalues, which is how the principal components are mathematically derived.             \n",
    "\n",
    "   * <e style=\"color:blue; font-weight:bold\">Orthogonality:</e> The principal components are orthogonal (perpendicular) to each other, meaning they are linearly uncorrelated.            \n",
    "\n",
    "\n",
    "   ---\n",
    "\n",
    "\n",
    "### This concept is visually demanding, we need to visualize conversion of 3-d (higher) space into a 2-D (lower) space to see the effects of PCA on data spreading.\n",
    "\n",
    "<style>\n",
    ".snap-container {\n",
    "    display: flex;\n",
    "    overflow-x: auto;\n",
    "    gap: 40px;\n",
    "    scroll-snap-type: x mandatory;\n",
    "    padding: 10px;\n",
    "    border: 1px solid #ccc;\n",
    "}\n",
    ".snap-container::-webkit-scrollbar {\n",
    "    height: 8px;\n",
    "    background: #750606ff;\n",
    "}\n",
    ".snap-container::-webkit-scrollbar-thumb {\n",
    "    background: #999;\n",
    "    border-radius: 4px;\n",
    "}\n",
    ".snap-item {\n",
    "    flex: 0 0 auto;\n",
    "    scroll-snap-align: center;\n",
    "    transition: transform 0.25s ease;\n",
    "}\n",
    ".snap-item:hover {\n",
    "    transform: scale(1.10);\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div class=\"snap-container\">\n",
    "    <img class=\"snap-item\" src=\"./assets/images/PCA_Reduction.jpeg\" style=\"height:600px; width:850px; margin-left:10px; margin-bottom:10px; margin-top:10px; padding:15px\">\n",
    "    <img class=\"snap-item\" src=\"./assets/images/Before_Vs_After_PCA.png\" style=\"height:600px; width:850px; margin-left:10px; margin-bottom:10px; margin-top:10px; padding:5px\">\n",
    "    <!-- <img class=\"snap-item\" src=\"./assets/images/KNN_3.png\" style=\"height:600px; width:850px; margin-left:20px; margin-bottom:20px; margin-top:20px\">\n",
    "    <img class=\"snap-item\" src=\"./assets/images/KNN_4.png\" style=\"height:600px; width:850px; margin-left:20px; margin-bottom:20px; margin-top:20px; margin-right:20px\"> -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfe52d4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
