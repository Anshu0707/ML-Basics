{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59f5bafa",
   "metadata": {},
   "source": [
    "# <span style=\"color:purple; font-weight:bold\"> Principal Component Anaylisis           \n",
    "\n",
    "- **PCA is a dimensionality reduction technique.**  \n",
    "                  \n",
    "- **In real life the datasets used in models have large number of features (dimensions) all of which contribute to predicting target variable.**   \n",
    "\n",
    "- **PCA reduces the number of features by finding new axes (called principal components) that capture the maximum variance in the data.**        \n",
    "\n",
    "- **Think of it as to “Rotate the dataset onto directions where the data varies the most, then keep only the strongest directions.”**             \n",
    "\n",
    "\n",
    "### <span style=\"color:purple; font-weight:bold\">Advantages ?</span>        \n",
    "\n",
    "* Speeds up ML models\n",
    "\n",
    "* Removes noise\n",
    "\n",
    "* Avoids multicollinearity\n",
    "\n",
    "* Helps visualize high-dimensional data\n",
    "\n",
    "* Makes patterns easier to see              \n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:purple; font-weight:bold\">Maths</span>     \n",
    "\n",
    "  Below are the mathemetical topics you must be familiar with to understand PCA.          \n",
    "\n",
    "- <ins style=\"color:purple; font-weight:bold\">Statistic</ins>\n",
    "   * <span style=\"color:blue; font-weight:bold\">Mean/Average:</span> Calculating the mean of each feature is the first step in centering the data, a critical preprocessing step for PCA.      \n",
    "\n",
    "   * <span style=\"color:blue; font-weight:bold\">Variance:</span> This measures the spread of data points along a single dimension. PCA's primary goal is to find new axes (principal components) that maximize this variance.      \n",
    "\n",
    "   * <span style=\"color:blue; font-weight:bold\">Covariance:</span> This measures the degree to which two variables change together. PCA aims to transform correlated features into new, uncorrelated components (zero covariance).    \n",
    "\n",
    "   * <span style=\"color:blue; font-weight:bold\">Covariance Matrix:</span> This symmetric matrix summarizes all the pairwise covariances and individual variances of your dataset's features. It is the central piece of the PCA calculation.      \n",
    "\n",
    "   \n",
    "- <ins style=\"color:purple; font-weight:bold\">Linear Alegebra</ins>\n",
    "   * <span style=\"color:blue; font-weight:bold\">Scalars, Vectors, and Matrices:</span> Basic familiarity with these data structures and their operations (addition, multiplication, transpose) is essential for handling data manipulation in the algorithm.             \n",
    "\n",
    "   * <span style=\"color:blue; font-weight:bold\">Matrix Operations:</span> Understanding matrix multiplication and transposition is necessary for working with the data matrix and the covariance matrix.   \n",
    "\n",
    "   * <span style=\"color:blue; font-weight:bold\">Eigenvectors:</span> represent the directions of the new axes (principal components) where the data has the most variance.      \n",
    "\n",
    "   * <span style=\"color:blue; font-weight:bold\">Eigenvalues:</span> indicate the magnitude of the variance along each eigenvector's direction. Larger eigenvalues mean more information is captured by that component.            \n",
    "\n",
    "   * <span style=\"color:blue; font-weight:bold\">Eigendecomposition:</span> This is the process of breaking down the covariance matrix into its eigenvectors and eigenvalues, which is how the principal components are mathematically derived.             \n",
    "\n",
    "   * <span style=\"color:blue; font-weight:bold\">Orthogonality:</span> The principal components are orthogonal (perpendicular) to each other, meaning they are linearly uncorrelated.            \n",
    "\n",
    "\n",
    "   ---\n",
    "\n",
    "\n",
    "### This concept is visually demanding, we need to visualize conversion of 3-d (higher) space into a 2-D (lower) space to see the effects of PCA on data spreading.\n",
    "\n",
    "<div style=\"\n",
    "    display:flex;\n",
    "    overflow-x:auto;\n",
    "    overflow-y:hidden;\n",
    "    gap:40px;\n",
    "    padding:10px;\n",
    "    border:1px solid #ccc;\n",
    "    white-space:nowrap;\n",
    "    align-items:center;\n",
    "\">\n",
    "\n",
    "  <img src=\"./assets/images/PCA_Reduction.jpeg\"\n",
    "       style=\"\n",
    "          height:450px;\n",
    "          width:700px;\n",
    "          max-width:none;\n",
    "          max-height:none;\n",
    "          margin:-10px;\n",
    "          padding:10px;\n",
    "          display:inline-block;\n",
    "       \">\n",
    "\n",
    "  <img src=\"./assets/images/Before_Vs_After_PCA.png\"\n",
    "       style=\"\n",
    "          height:450px;\n",
    "          width:700px;\n",
    "          max-width:none;\n",
    "          max-height:none;\n",
    "          margin:3px;\n",
    "          padding:10px;\n",
    "          display:inline-block;\n",
    "       \">\n",
    "\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbf9b03",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
